{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzV2hamjVJ/VS1SfdjNu9+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gomzkevin/kontempo/blob/main/early_warnings_4_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Pipeline Completo y Unificado del Modelo de Alertas Tempranas.\n",
        "\n",
        "Versión Definitiva.\n",
        "\n",
        "Este script integra la lógica de cálculo validada por la auditoría para\n",
        "garantizar la precisión y robustez. Resuelve los errores de acumulación\n",
        "y los KeyErrors.\n",
        "\n",
        "El proceso es:\n",
        "1.  Construcción del Dataset de Análisis con lógica de cálculo robusta.\n",
        "2.  Entrenamiento del Modelo RandomForest.\n",
        "3.  Scoring del Portafolio Actual con la lógica de negocio final.\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from IPython.display import display\n",
        "\n",
        "print(\"Iniciando el pipeline completo del modelo...\")\n",
        "\n",
        "# =============================================================================\n",
        "# PASO 1: CONSTRUCCIÓN DEL DATASET DE ANÁLISIS\n",
        "# =============================================================================\n",
        "print(\"\\n--- [PASO 1] Iniciando la construcción del dataset de análisis... ---\")\n",
        "\n",
        "# --- 1.1 Carga y Preparación de Datos ---\n",
        "payments_df = pd.DataFrame()\n",
        "file_path = 'payments.json'\n",
        "\n",
        "try:\n",
        "    print(f\"Cargando y preparando los datos desde '{file_path}'...\")\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f: data = json.load(f)\n",
        "    except json.JSONDecodeError:\n",
        "        data = []\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                if line.strip():\n",
        "                    try: data.append(json.loads(line.strip().rstrip(',')))\n",
        "                    except json.JSONDecodeError: continue\n",
        "\n",
        "    payments_df_raw = pd.DataFrame(data)\n",
        "    payments_df = payments_df_raw.copy()\n",
        "\n",
        "    if not payments_df.empty:\n",
        "        rename_map = {\n",
        "            'ID': 'id', 'Pkey': 'pkey', 'Amount Due': 'amount_due', 'Amount Paid': 'amount_paid',\n",
        "            'Completed Status': 'completed_status', 'Created': 'created', 'Loan ID': 'loan_id',\n",
        "            'Due Date': 'due_date', 'Principal Amount': 'principal_amount', 'Buyer Account': 'buyer_account',\n",
        "            'Payment Number': 'payment_number', 'Status': 'status'\n",
        "        }\n",
        "        payments_df.rename(columns=lambda c: rename_map.get(c, c.lower().replace(' ', '_')), inplace=True)\n",
        "        if 'status' in payments_df.columns:\n",
        "            payments_df = payments_df[payments_df['status'] != 'voided'].copy()\n",
        "        numeric_cols = ['amount_due', 'amount_paid', 'principal_amount', 'payment_number', 'created']\n",
        "        for col in numeric_cols:\n",
        "            payments_df[col] = pd.to_numeric(payments_df[col].astype(str).str.replace(',', ''), errors='coerce')\n",
        "        payments_df['created_date'] = pd.to_datetime(payments_df['created'], unit='s', errors='coerce')\n",
        "        payments_df['due_date'] = pd.to_datetime(payments_df['due_date'], errors='coerce')\n",
        "        payments_df.dropna(subset=['created_date', 'loan_id', 'buyer_account'], inplace=True)\n",
        "        print(\"✅ Datos limpios y preparados.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ ERROR CRÍTICO: No se encontró el archivo de datos '{file_path}'.\")\n",
        "    payments_df = pd.DataFrame()\n",
        "\n",
        "# El resto del pipeline solo se ejecuta si los datos se cargaron\n",
        "if not payments_df.empty:\n",
        "    # --- 1.2 Enriquecimiento con Límite de Crédito ---\n",
        "    print(\"Cargando y fusionando datos de límites de crédito...\")\n",
        "    try:\n",
        "        limits_df = pd.read_csv('Limit - Credit.csv')\n",
        "        limits_df.columns = limits_df.columns.str.strip()\n",
        "        limits_df = limits_df[['Buyer Account ID', 'Limit']]\n",
        "        limits_df.columns = ['buyer_account', 'limite_de_credito']\n",
        "\n",
        "        limits_df['limite_de_credito'] = pd.to_numeric(limits_df['limite_de_credito'].astype(str).str.replace('[$,]', '', regex=True), errors='coerce')\n",
        "        aggregated_limits = limits_df.groupby('buyer_account')['limite_de_credito'].sum().reset_index()\n",
        "\n",
        "        payments_df = pd.merge(payments_df, aggregated_limits, on='buyer_account', how='left')\n",
        "\n",
        "        loan_level_df_temp = payments_df.groupby('loan_id').agg(buyer_account=('buyer_account', 'first'), principal_amount=('principal_amount', 'first')).reset_index()\n",
        "        max_principal_by_buyer = loan_level_df_temp.groupby('buyer_account')['principal_amount'].max()\n",
        "        payments_df = pd.merge(payments_df, max_principal_by_buyer.rename('max_principal_imputed'), on='buyer_account', how='left')\n",
        "\n",
        "        payments_df.loc[payments_df['limite_de_credito'].isnull(), 'limite_de_credito'] = payments_df['max_principal_imputed']\n",
        "        payments_df.loc[payments_df['limite_de_credito'] == 0, 'limite_de_credito'] = payments_df['max_principal_imputed']\n",
        "        payments_df.drop(columns=['max_principal_imputed'], inplace=True)\n",
        "        print(\"✅ Límites de crédito enriquecidos y fusionados.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"⚠️ ADVERTENCIA: No se encontró 'Limit - Credit.csv'. Se imputará el límite con el monto máximo dispuesto.\")\n",
        "        loan_level_df_temp = payments_df.groupby('loan_id').agg(buyer_account=('buyer_account', 'first'), principal_amount=('principal_amount', 'first')).reset_index()\n",
        "        max_principal_by_buyer = loan_level_df_temp.groupby('buyer_account')['principal_amount'].transform('max')\n",
        "        payments_df['limite_de_credito'] = max_principal_by_buyer\n",
        "\n",
        "    # --- 1.3 Cálculo Iterativo de Métricas ---\n",
        "    print(\"Creando la estructura del dataset y calculando métricas mensuales...\")\n",
        "    loan_level_df = payments_df.groupby('loan_id').agg(\n",
        "        issuance_date=('created_date', 'min'), principal_amount=('principal_amount', 'first'),\n",
        "        buyer_account=('buyer_account', 'first'), total_installments=('payment_number', 'max')\n",
        "    ).reset_index()\n",
        "    loan_level_df['total_installments'] = pd.to_numeric(loan_level_df['total_installments'], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "    min_date = payments_df['created_date'].min().to_period('M').to_timestamp(how='end')\n",
        "    max_date = (payments_df['created_date'].max() - pd.DateOffset(days=70)).to_period('M').to_timestamp(how='end')\n",
        "    snapshot_dates = pd.date_range(min_date, max_date, freq='ME')\n",
        "\n",
        "    all_snapshots = []\n",
        "    materiality_threshold = 0.04\n",
        "    epsilon = 1e-6\n",
        "\n",
        "    for snapshot_date in snapshot_dates:\n",
        "        history_payments = payments_df[payments_df['created_date'] <= snapshot_date].copy()\n",
        "        history_loans = loan_level_df[loan_level_df['issuance_date'] <= snapshot_date]\n",
        "\n",
        "        active_buyers_in_snapshot = history_loans[history_loans['issuance_date'] >= (snapshot_date - pd.DateOffset(days=180))]['buyer_account'].unique()\n",
        "        current_snapshot = pd.DataFrame(active_buyers_in_snapshot, columns=['buyer_account'])\n",
        "        if current_snapshot.empty: continue\n",
        "        current_snapshot['snapshot_date'] = snapshot_date\n",
        "\n",
        "        # --- LÓGICA DE CÁLCULO DEFINITIVA Y ROBUSTA (TRASPLANTADA DE LA AUDITORÍA) ---\n",
        "        hp = history_payments\n",
        "        hp['amount_due_clean'] = hp['amount_due'].fillna(0)\n",
        "        hp['amount_paid_clean'] = hp['amount_paid'].fillna(0)\n",
        "        hp['saldo_individual'] = hp['amount_due_clean'] - hp['amount_paid_clean']\n",
        "\n",
        "        with np.errstate(divide='ignore', invalid='ignore'):\n",
        "            hp['paid_ratio'] = hp['amount_paid_clean'].divide(hp['amount_due_clean'])\n",
        "            hp['paid_ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "            hp['paid_ratio'].fillna(1, inplace=True) # Asumir 100% pagado si 'due' es 0 o nulo\n",
        "\n",
        "        hp['is_open'] = (\n",
        "            (hp['saldo_individual'] > epsilon) &\n",
        "            (hp['amount_due_clean'] > epsilon) &\n",
        "            (hp['paid_ratio'] < (1 - materiality_threshold))\n",
        "        )\n",
        "\n",
        "        past_due_df = hp[(hp['is_open']) & (hp['due_date'] < snapshot_date)]\n",
        "        if not past_due_df.empty:\n",
        "            max_dpd_series = past_due_df.groupby('buyer_account')['due_date'].max()\n",
        "            max_dpd = (snapshot_date - max_dpd_series).dt.days\n",
        "            current_snapshot = current_snapshot.merge(max_dpd.rename('max_dpd_actual'), on='buyer_account', how='left')\n",
        "\n",
        "        if 'max_dpd_actual' not in current_snapshot.columns:\n",
        "            current_snapshot['max_dpd_actual'] = 0\n",
        "        current_snapshot['max_dpd_actual'] = current_snapshot['max_dpd_actual'].fillna(0)\n",
        "\n",
        "        current_snapshot = current_snapshot[current_snapshot['max_dpd_actual'] <= 35]\n",
        "        if current_snapshot.empty: continue\n",
        "\n",
        "        saldo_pendiente = hp[hp['is_open']].groupby('buyer_account')['saldo_individual'].sum()\n",
        "        current_snapshot = current_snapshot.merge(saldo_pendiente.rename('saldo_pendiente_actual'), on='buyer_account', how='left')\n",
        "\n",
        "        # --- CÁLCULO DE OTRAS FEATURES ---\n",
        "        acquisition_dates = history_payments.groupby('buyer_account')['created_date'].min()\n",
        "        current_snapshot = current_snapshot.merge(acquisition_dates.rename('acquisition_date'), on='buyer_account', how='left')\n",
        "        current_snapshot['antiguedad_cliente_meses'] = ((snapshot_date.year - current_snapshot['acquisition_date'].dt.year) * 12 + (snapshot_date.month - current_snapshot['acquisition_date'].dt.month))\n",
        "        num_loans = history_loans.groupby('buyer_account')['loan_id'].nunique()\n",
        "        current_snapshot = current_snapshot.merge(num_loans.rename('numero_total_prestamos_historico'), on='buyer_account', how='left')\n",
        "        loans_30d = history_loans[history_loans['issuance_date'] >= (snapshot_date - pd.DateOffset(days=30))]\n",
        "        monto_30d = loans_30d.groupby('buyer_account')['principal_amount'].sum()\n",
        "        freq_30d = loans_30d.groupby('buyer_account')['loan_id'].nunique()\n",
        "        current_snapshot = current_snapshot.merge(monto_30d.rename('monto_dispuesto_ultimos_30d'), on='buyer_account', how='left')\n",
        "        current_snapshot = current_snapshot.merge(freq_30d.rename('frecuencia_prestamos_ultimos_30d'), on='buyer_account', how='left')\n",
        "        loans_31_60d = history_loans[(history_loans['issuance_date'] >= (snapshot_date - pd.DateOffset(days=60))) & (history_loans['issuance_date'] < (snapshot_date - pd.DateOffset(days=30)))]\n",
        "        monto_31_60d = loans_31_60d.groupby('buyer_account')['principal_amount'].sum()\n",
        "        freq_31_60d = loans_31_60d.groupby('buyer_account')['loan_id'].nunique()\n",
        "        current_snapshot = current_snapshot.merge(monto_31_60d.rename('monto_dispuesto_31_60d'), on='buyer_account', how='left')\n",
        "        current_snapshot = current_snapshot.merge(freq_31_60d.rename('frecuencia_prestamos_31_60d'), on='buyer_account', how='left')\n",
        "\n",
        "        limites = history_payments.groupby('buyer_account')['limite_de_credito'].first()\n",
        "        current_snapshot = current_snapshot.merge(limites.rename('limite_de_credito'), on='buyer_account', how='left')\n",
        "        current_snapshot['porcentaje_utilizacion'] = (current_snapshot['saldo_pendiente_actual'] / current_snapshot['limite_de_credito'].replace(0, np.nan)) * 100\n",
        "        if not history_loans.empty:\n",
        "            most_recent_loan = history_loans.loc[history_loans.groupby('buyer_account')['issuance_date'].idxmax()]\n",
        "            loan_terms = most_recent_loan[['buyer_account', 'total_installments']].rename(columns={'total_installments': 'installments_prestamo_reciente'})\n",
        "            current_snapshot = current_snapshot.merge(loan_terms, on='buyer_account', how='left')\n",
        "            avg_installments = history_loans.groupby('buyer_account')['total_installments'].mean()\n",
        "            current_snapshot = current_snapshot.merge(avg_installments.rename('promedio_installments_historico'), on='buyer_account', how='left')\n",
        "\n",
        "        current_snapshot.fillna(0, inplace=True)\n",
        "        current_snapshot['es_primer_mes_activo'] = (current_snapshot['antiguedad_cliente_meses'] == 0).astype(int)\n",
        "        current_snapshot['aceleracion_monto'] = current_snapshot['monto_dispuesto_ultimos_30d'] - current_snapshot['monto_dispuesto_31_60d']\n",
        "        current_snapshot['aceleracion_frecuencia'] = current_snapshot['frecuencia_prestamos_ultimos_30d'] - current_snapshot['frecuencia_prestamos_31_60d']\n",
        "        current_snapshot['cambio_en_installments_reciente'] = current_snapshot['installments_prestamo_reciente'] - current_snapshot['promedio_installments_historico']\n",
        "\n",
        "        future_payments = payments_df[(payments_df['due_date'] > snapshot_date) & (payments_df['due_date'] <= snapshot_date + pd.Timedelta(days=90))].copy()\n",
        "        future_payments['default_check_date'] = future_payments['due_date'] + pd.Timedelta(days=35)\n",
        "        future_payments['saldo_individual'] = future_payments['amount_due'].fillna(0) - future_payments['amount_paid'].fillna(0)\n",
        "        future_payments['is_open'] = (future_payments['saldo_individual'] > epsilon) & (future_payments['amount_due'] > epsilon) & ((future_payments['amount_paid'].fillna(0) / future_payments['amount_due']) < (1 - materiality_threshold))\n",
        "        defaulted_payments = future_payments[(future_payments['default_check_date'] < pd.Timestamp.now()) & (future_payments['is_open'])]\n",
        "        defaulted_buyers = defaulted_payments['buyer_account'].unique()\n",
        "        current_snapshot['default_en_35d'] = np.where(current_snapshot['buyer_account'].isin(defaulted_buyers), 1, 0)\n",
        "\n",
        "        all_snapshots.append(current_snapshot)\n",
        "\n",
        "    final_dataset = pd.concat(all_snapshots, ignore_index=True)\n",
        "    final_dataset.fillna(0, inplace=True)\n",
        "    final_dataset = final_dataset[final_dataset['snapshot_date'] >= final_dataset['acquisition_date']]\n",
        "    print(f\"✅ Dataset de análisis finalizado con {len(final_dataset)} filas.\")\n",
        "\n",
        "    # =============================================================================\n",
        "    # PASO 2: ENTRENAMIENTO DEL MODELO\n",
        "    # =============================================================================\n",
        "    print(\"\\n--- [PASO 2] Iniciando el entrenamiento del modelo... ---\")\n",
        "    features = [col for col in final_dataset.columns if col not in ['buyer_account', 'snapshot_date', 'acquisition_date', 'default_en_35d']]\n",
        "    target = 'default_en_35d'\n",
        "\n",
        "    X = final_dataset[features]\n",
        "    y = final_dataset[target]\n",
        "\n",
        "    split_date = '2024-12-31'\n",
        "    train_mask = final_dataset['snapshot_date'] <= pd.to_datetime(split_date)\n",
        "    test_mask = final_dataset['snapshot_date'] > pd.to_datetime(split_date)\n",
        "\n",
        "    X_train, X_test = X[train_mask], X[test_mask]\n",
        "    y_train, y_test = y[train_mask], y[test_mask]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    model = RandomForestClassifier(n_estimators=100, class_weight='balanced_subsample', random_state=42, n_jobs=-1)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    print(\"✅ Modelo entrenado exitosamente.\")\n",
        "\n",
        "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    print(f\"📈 Desempeño del modelo en datos de prueba (AUC): {auc:.4f}\")\n",
        "\n",
        "\n",
        "    # =============================================================================\n",
        "    # PASO 3: SCORING DEL PORTAFOLIO ACTUAL\n",
        "    # =============================================================================\n",
        "    print(\"\\n--- [PASO 3] Iniciando el scoring del portafolio actual... ---\")\n",
        "\n",
        "    def recalculate_features_for_buyer(buyer_id, all_payments_df, all_loans_df, today_date, materiality_thresh, epsilon_val):\n",
        "        client_payments = all_payments_df[all_payments_df['buyer_account'] == buyer_id].copy()\n",
        "        client_loans = all_loans_df[all_loans_df['buyer_account'] == buyer_id].copy()\n",
        "        if client_payments.empty: return None\n",
        "\n",
        "        client_df = pd.DataFrame([{'buyer_account': buyer_id}])\n",
        "\n",
        "        acquisition_date = client_payments['created_date'].min()\n",
        "        client_df['acquisition_date'] = acquisition_date\n",
        "        client_df['antiguedad_cliente_meses'] = ((today_date.year - acquisition_date.year) * 12 + (today_date.month - acquisition_date.month))\n",
        "        client_df['es_primer_mes_activo'] = (client_df['antiguedad_cliente_meses'] == 0).astype(int)\n",
        "        client_df['numero_total_prestamos_historico'] = client_loans['loan_id'].nunique()\n",
        "\n",
        "        loans_30d = client_loans[client_loans['issuance_date'] >= (today_date - pd.DateOffset(days=30))]\n",
        "        client_df['monto_dispuesto_ultimos_30d'] = loans_30d['principal_amount'].sum()\n",
        "        client_df['frecuencia_prestamos_ultimos_30d'] = loans_30d['loan_id'].nunique()\n",
        "\n",
        "        loans_31_60d = client_loans[(client_loans['issuance_date'] >= (today_date - pd.DateOffset(days=60))) & (client_loans['issuance_date'] < (today_date - pd.DateOffset(days=30)))]\n",
        "        client_df['monto_dispuesto_31_60d'] = loans_31_60d['principal_amount'].sum()\n",
        "        client_df['frecuencia_prestamos_31_60d'] = loans_31_60d['loan_id'].nunique()\n",
        "\n",
        "        pay_df = client_payments.copy()\n",
        "        pay_df['amount_due_clean'] = pd.to_numeric(pay_df['amount_due'], errors='coerce').fillna(0.0)\n",
        "        pay_df['amount_paid_clean'] = pd.to_numeric(pay_df['amount_paid'], errors='coerce').fillna(0.0)\n",
        "        pay_df['saldo_individual'] = pay_df['amount_due_clean'] - pay_df['amount_paid_clean']\n",
        "        with np.errstate(divide='ignore', invalid='ignore'):\n",
        "            pay_df['paid_ratio'] = pay_df['amount_paid_clean'].divide(pay_df['amount_due_clean'])\n",
        "            pay_df['paid_ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "            pay_df['paid_ratio'].fillna(1, inplace=True)\n",
        "        pay_df['is_open'] = (\n",
        "            (pay_df['saldo_individual'] > epsilon_val) &\n",
        "            (pay_df['amount_due_clean'] > epsilon_val) &\n",
        "            (pay_df['paid_ratio'] < (1 - materiality_thresh))\n",
        "        )\n",
        "\n",
        "        client_df['saldo_pendiente_actual'] = pay_df.loc[pay_df['is_open'], 'saldo_individual'].sum()\n",
        "\n",
        "        past_due_df = pay_df[(pay_df['is_open']) & (pay_df['due_date'] < today_date)]\n",
        "        client_df['max_dpd_actual'] = (today_date - past_due_df['due_date']).dt.days.max() if not past_due_df.empty else 0\n",
        "\n",
        "        client_df['limite_de_credito'] = client_payments['limite_de_credito'].iloc[0] if not client_payments['limite_de_credito'].isnull().all() else 0\n",
        "        client_df['porcentaje_utilizacion'] = (client_df['saldo_pendiente_actual'] / client_df['limite_de_credito'].replace(0, np.nan)) * 100\n",
        "\n",
        "        if not client_loans.empty:\n",
        "            most_recent_loan = client_loans.loc[client_loans['issuance_date'].idxmax()]\n",
        "            client_df['installments_prestamo_reciente'] = most_recent_loan['total_installments']\n",
        "            client_df['promedio_installments_historico'] = client_loans['total_installments'].mean()\n",
        "        else:\n",
        "            client_df['installments_prestamo_reciente'] = 0\n",
        "            client_df['promedio_installments_historico'] = 0\n",
        "\n",
        "        client_df.fillna(0, inplace=True)\n",
        "        client_df['aceleracion_monto'] = client_df['monto_dispuesto_ultimos_30d'] - client_df['monto_dispuesto_31_60d']\n",
        "        client_df['aceleracion_frecuencia'] = client_df['frecuencia_prestamos_ultimos_30d'] - client_df['frecuencia_prestamos_31_60d']\n",
        "        client_df['cambio_en_installments_reciente'] = client_df['installments_prestamo_reciente'] - client_df['promedio_installments_historico']\n",
        "\n",
        "        return client_df\n",
        "\n",
        "    all_unique_buyers = payments_df['buyer_account'].unique()\n",
        "    scoring_results = []\n",
        "    today = payments_df['created_date'].max()\n",
        "\n",
        "    print(\"Calculando features para el portafolio actual...\")\n",
        "    for buyer_id in all_unique_buyers:\n",
        "        client_features = recalculate_features_for_buyer(buyer_id, payments_df, loan_level_df, today, materiality_threshold, epsilon)\n",
        "        if client_features is not None:\n",
        "            scoring_results.append(client_features)\n",
        "\n",
        "    scoring_df = pd.concat(scoring_results, ignore_index=True)\n",
        "\n",
        "    scoring_df.sort_values('buyer_account', inplace=True)\n",
        "    scoring_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    for col in features:\n",
        "        if col not in scoring_df.columns:\n",
        "            scoring_df[col] = 0\n",
        "\n",
        "    X_today = scoring_df[features]\n",
        "    X_today_scaled = scaler.transform(X_today)\n",
        "    risk_scores = model.predict_proba(X_today_scaled)[:, 1]\n",
        "    scoring_df['risk_score'] = risk_scores\n",
        "    print(\"✅ Scores de riesgo calculados para todo el portafolio.\")\n",
        "\n",
        "    print(\"Aplicando segmentación y reglas de negocio...\")\n",
        "    bins = [-0.01, 0.10, 0.40, 0.70, 1.01]\n",
        "    labels = ['Bajo', 'Medio', 'Alto', 'Crítico']\n",
        "    scoring_df['nivel_de_riesgo_modelo'] = pd.cut(scoring_df['risk_score'], bins=bins, labels=labels)\n",
        "\n",
        "    scoring_df['segmento_final'] = scoring_df['nivel_de_riesgo_modelo'].astype(str)\n",
        "\n",
        "    last_loan_dates = loan_level_df.groupby('buyer_account')['issuance_date'].max()\n",
        "    scoring_df = pd.merge(scoring_df, last_loan_dates.rename('last_loan_date'), on='buyer_account', how='left')\n",
        "    scoring_df['dias_desde_ultimo_prestamo'] = (today - scoring_df['last_loan_date']).dt.days\n",
        "    scoring_df.loc[scoring_df['dias_desde_ultimo_prestamo'] > 180, 'segmento_final'] = 'Inactivo'\n",
        "\n",
        "    good_history_mask = (scoring_df['segmento_final'] == 'Crítico') & \\\n",
        "                        (scoring_df['max_dpd_actual'] == 0) & \\\n",
        "                        (scoring_df['numero_total_prestamos_historico'] > 5)\n",
        "    scoring_df.loc[good_history_mask, 'segmento_final'] = 'Alto (Revisión por Regla)'\n",
        "\n",
        "    scoring_df.loc[scoring_df['max_dpd_actual'] > 35, 'segmento_final'] = 'En Cobranza'\n",
        "    scoring_df.fillna({'segmento_final': 'Bajo'}, inplace=True)\n",
        "\n",
        "    all_report_cols = [\n",
        "        'buyer_account', 'risk_score', 'segmento_final', 'nivel_de_riesgo_modelo', 'dias_desde_ultimo_prestamo',\n",
        "        'limite_de_credito', 'acquisition_date', 'antiguedad_cliente_meses', 'es_primer_mes_activo',\n",
        "        'numero_total_prestamos_historico', 'monto_dispuesto_ultimos_30d', 'frecuencia_prestamos_ultimos_30d',\n",
        "        'monto_dispuesto_31_60d', 'frecuencia_prestamos_31_60d', 'saldo_pendiente_actual',\n",
        "        'porcentaje_utilizacion', 'max_dpd_actual', 'installments_prestamo_reciente',\n",
        "        'promedio_installments_historico', 'aceleracion_monto', 'aceleracion_frecuencia',\n",
        "        'cambio_en_installments_reciente'\n",
        "    ]\n",
        "    final_cols_to_save = [col for col in all_report_cols if col in scoring_df.columns]\n",
        "    scoring_df[final_cols_to_save].to_csv('reporte_scoring_completo_unificado.csv', index=False)\n",
        "    print(\"✅ Reporte de scoring completo guardado.\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*20 + \" Resumen Final del Portafolio por Riesgo \" + \"=\"*20)\n",
        "    risk_summary = scoring_df.groupby('segmento_final', observed=False).agg(\n",
        "        Numero_de_Clientes=('buyer_account', 'count'),\n",
        "        Score_Promedio=('risk_score', 'mean')\n",
        "    ).sort_index()\n",
        "    risk_summary['Score_Promedio'] = (risk_summary['Score_Promedio'] * 100).map('{:.2f}%'.format)\n",
        "    display(risk_summary)\n",
        "\n",
        "    print(\"\\n\\n✅ Pipeline completo finalizado.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nPipeline detenido porque no se cargaron datos.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAzefarbqAcq",
        "outputId": "01d0b262-77f4-47d6-85c0-d41d8c7762d8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando el pipeline completo del modelo...\n",
            "\n",
            "--- [PASO 1] Iniciando la construcción del dataset de análisis... ---\n",
            "Cargando y preparando los datos desde 'payments.json'...\n",
            "❌ ERROR CRÍTICO: No se encontró el archivo de datos 'payments.json'.\n",
            "\n",
            "Pipeline detenido porque no se cargaron datos.\n"
          ]
        }
      ]
    }
  ]
}